{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Steg-RoBERTa.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSfDk5CxUKbg",
        "outputId": "f26e8f6b-04a1-4d73-83ab-3ffe7512d6a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.21.1-py3-none-any.whl (4.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 7.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 4.0 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 35.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 20.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.8.1 pyyaml-6.0 tokenizers-0.12.1 transformers-4.21.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 7.9 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.97\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wordpiece\n",
            "  Downloading wordpiece-0.0.0.tar.gz (2.1 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: wordpiece\n",
            "  Building wheel for wordpiece (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wordpiece: filename=wordpiece-0.0.0-py3-none-any.whl size=1999 sha256=2b13f8460387fd24b6c6177afc1ec21531618f4e631d35b3a2bbf168babdd4e5\n",
            "  Stored in directory: /root/.cache/pip/wheels/a9/dd/42/24eed7273d41969aab38c87fce5a092bf1bdfe1470450d4dd2\n",
            "Successfully built wordpiece\n",
            "Installing collected packages: wordpiece\n",
            "Successfully installed wordpiece-0.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "!pip install wordpiece\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile roBERTa.py\n",
        "\n",
        "from typing import List, Tuple, Union\n",
        "from io import StringIO\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import torch\n",
        "from torch import Tensor\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers.tokenization_utils import PreTrainedTokenizer\n",
        "from transformers import RobertaTokenizer, RobertaForMaskedLM\n",
        "import heapq\n",
        "from heapq import heappop, heappush\n",
        "\n",
        "class MaskedStego:\n",
        "  \n",
        "\n",
        "\n",
        "    def __init__(self,name = 'roberta-base'): #cased bert means that uppercase, accent markers are accepted \n",
        "        self._tokenizer: PreTrainedTokenizer = RobertaTokenizer.from_pretrained(name)\n",
        "        self._model = RobertaForMaskedLM.from_pretrained(name)\n",
        "        self._STOPWORDS: List[str] = stopwords.words('english')\n",
        "        \n",
        "\n",
        "    def __call__(self, cover_text, message, mask_interval = 4, score_threshold = 0.01): \n",
        "        assert set(message) <= set('01') #make sure that the binary message is a set of 0s and 1s\n",
        "        message_io = StringIO(message) \n",
        "        #pre-processing cover text inserted \n",
        "        #encode process goes through 4 steps normalization of cover text, pre-tokenizing, model, decoding. (similar to tokenizer.encode)\n",
        "        encoded_ids= self._tokenizer([cover_text],return_tensors='pt').input_ids[0] #returns a tensor for each word of the cover text, tensor is numerical representation for each word within the cover text\n",
        "        \n",
        "        masked_ids= self._mask(encoded_ids.clone(),mask_interval) #masked ids are the position embeddings for each word\n",
        "        sorted_score, indices= self._predict(masked_ids)\n",
        "        \n",
        "\n",
        "        processed= { 'input_ids': encoded_ids, 'masked_ids': masked_ids, 'sorted_output': (sorted_score, indices) }\n",
        "        \n",
        "        input_ids = processed['input_ids'] \n",
        "        masked_ids = processed['masked_ids'] #1 for words and 0 for paddings\n",
        "        sorted_score, indices = processed['sorted_output']\n",
        "        for i_token, token in enumerate(masked_ids):\n",
        "            if token != self._tokenizer.mask_token_id:\n",
        "                continue\n",
        "            ids = indices[i_token]\n",
        "            scores = sorted_score[i_token]\n",
        "            #picks candidates according to their scores that must be of probability more than the set threshold 0.01 for encoding and 0.005 for decoding\n",
        "            candidates = self._pick_candidates_threshold(ids, scores, score_threshold)\n",
        "            print(candidates)\n",
        "            print(self._tokenizer.convert_ids_to_tokens(candidates)) #all suitable words to be placed in the [mask] position\n",
        "            \n",
        "            replace_token_id = self._block_encode_single(candidates, message_io).item() #perfect binary tree btakhod el candidates w btkhtar el suitable words for the bit sequence of our secret message\n",
        "            \n",
        "            print('replace', replace_token_id, self._tokenizer.convert_ids_to_tokens([replace_token_id]))\n",
        "            input_ids[i_token] = replace_token_id #hena byghyar el [Mask] tokens into the replace_token_id tensor to be converted to tokens in the decode part coming (tokenizer.decode)\n",
        "            \n",
        "        encoded_message: str = message_io.getvalue()[:message_io.tell()]\n",
        "        \n",
        "        message_io.close()\n",
        "        stego_text = self._tokenizer.decode(input_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "        #self._roBERTa(cover_text,message);\n",
        "        \n",
        "        return { 'stego_text by roBERTa': stego_text, 'encoded_message': encoded_message }\n",
        "\n",
        "   \n",
        "\n",
        "    def _mask(self, input_ids: Union[Tensor, List[List[int]]], mask_interval: int) -> Tensor:\n",
        "        length = len(input_ids)\n",
        "        tokens: List[str] = self._tokenizer.convert_ids_to_tokens(input_ids)\n",
        "        \n",
        "        offset = mask_interval // 2 #the offset specify the number of [mask] tokens according to the specified mask_interval \n",
        "        \n",
        "        mask_count = offset\n",
        "        for i, token in enumerate(tokens):\n",
        "            # Skip initial subword\n",
        "            if i + 1 < length and tokens[i + 1].startswith('##'): continue\n",
        "            if not self._substitutable_single(token): continue #skipping stopwords, subwords, and words that are not within the (a-z) letters.\n",
        "            if mask_count % mask_interval == 0: \n",
        "                input_ids[i] = self._tokenizer.mask_token_id #replace the value of this index (normal word) with the [mask] token whose input_id equals to 103   \n",
        "            mask_count += 1  \n",
        "       \n",
        "        return input_ids\n",
        "\n",
        "    \n",
        "    #forward neural network layer\n",
        "    def _predict(self, input_ids: Union[Tensor, List[List[int]]]): #this is the MLM (masked language model)\n",
        "        self._model.eval()\n",
        "        with torch.no_grad():\n",
        "            output = self._model(input_ids.unsqueeze(0))['logits'][0] #The predicted token_id is extracted from this logit (embedding vector resulting from last encoder layer in bert) using a softmax transformation.\n",
        "            #softmaxed score is applying softmax on the scores to focus on the ones with high scores\n",
        "            softmaxed_score = F.softmax(output, dim=1)  # [word_len, vocab_len]\n",
        "            return softmaxed_score.sort(dim=1, descending=True)\n",
        "\n",
        "    \n",
        "   \n",
        "    def _pick_candidates_threshold(self, ids: Tensor, scores: Tensor, threshold: float) -> List[int]:\n",
        "        filtered_ids: List[int] = ids[scores >= threshold]\n",
        "        def filter_fun(idx: Tensor) -> bool:\n",
        "            \n",
        "            return self._substitutable_single(self._tokenizer.convert_ids_to_tokens(idx.item()))\n",
        "        return list(filter(filter_fun, filtered_ids))\n",
        "\n",
        "    def _substitutable_single(self, token: str) -> bool:\n",
        "        if token.startswith('##'): return False\n",
        "        if token.lower() in self._STOPWORDS: return False\n",
        "        if not token.isalpha(): return False\n",
        "        return True\n",
        "\n",
        "    @staticmethod\n",
        "    def _block_encode_single(ids: List[int], message: StringIO) -> int:  #Indexing\n",
        "        assert len(ids) > 0\n",
        "        if len(ids) == 1:\n",
        "            return ids[0]\n",
        "        capacity = len(ids).bit_length() - 1\n",
        "        #print(capacity)\n",
        "        bits_str = message.read(capacity)\n",
        "        print(\"part of bit sequence\",bits_str)\n",
        "        if len(bits_str) < capacity:\n",
        "            padding: str = '0' * (capacity - len(bits_str))\n",
        "            bits_str = bits_str + padding\n",
        "            message.write(padding)\n",
        "        index = int(bits_str, 2) #from binary to number\n",
        "        print(\"index\",index)\n",
        "        return ids[index]\n",
        "\n",
        "\n",
        "#Huffman Encoding \n",
        " \n",
        "def isLeaf(root):\n",
        "    return root.left is None and root.right is None\n",
        " \n",
        " \n",
        "# A Tree node\n",
        "class Node:\n",
        "    def __init__(self, ch, freq, left=None, right=None):\n",
        "        self.ch = ch\n",
        "        self.freq = freq\n",
        "        self.left = left\n",
        "        self.right = right\n",
        " \n",
        "    \n",
        "    def __lt__(self, other):\n",
        "        return self.freq < other.freq\n",
        " \n",
        " \n",
        "# Traverse the Huffman Tree and store Huffman Codes in a dictionary\n",
        "def encode(root, s, huffman_code):\n",
        " \n",
        "    if root is None:\n",
        "        return\n",
        " \n",
        "    # found a leaf node\n",
        "    if isLeaf(root):\n",
        "        huffman_code[root.ch] = s if len(s) > 0 else '1'\n",
        " \n",
        "    encode(root.left, s + '0', huffman_code)\n",
        "    encode(root.right, s + '1', huffman_code)\n",
        " \n",
        " \n",
        "\n",
        " \n",
        " \n",
        "# Builds Huffman Tree and decodes the given input text\n",
        "def buildHuffmanTree(text):\n",
        " \n",
        "    # base case: empty string\n",
        "    if len(text) == 0:\n",
        "        return\n",
        " \n",
        "    # count the frequency of appearance of each character\n",
        "    # and store it in a dictionary\n",
        "    freq = {i: text.count(i) for i in set(text)}\n",
        " \n",
        "    # Create a priority queue to store live nodes of the Huffman tree.\n",
        "    pq = [Node(k, v) for k, v in freq.items()]\n",
        "    heapq.heapify(pq)\n",
        " \n",
        "    # do till there is more than one node in the queue\n",
        "    while len(pq) != 1:\n",
        " \n",
        "        # Remove the two nodes of the highest priority\n",
        "        # (the lowest frequency) from the queue\n",
        " \n",
        "        left = heappop(pq)\n",
        "        right = heappop(pq)\n",
        " \n",
        "        # create a new internal node with these two nodes as children and\n",
        "        # with a frequency equal to the sum of the two nodes' frequencies.\n",
        "        # Add the new node to the priority queue.\n",
        " \n",
        "        total = left.freq + right.freq\n",
        "        heappush(pq, Node(None, total, left, right))\n",
        " \n",
        "    # `root` stores pointer to the root of Huffman Tree\n",
        "    root = pq[0]\n",
        " \n",
        "    # traverse the Huffman tree and store the Huffman codes in a dictionary\n",
        "    huffmanCode = {}\n",
        "    encode(root, '', huffmanCode)\n",
        " \n",
        " \n",
        "    # print the encoded string\n",
        "    s = ''\n",
        "    for c in text:\n",
        "        s += huffmanCode.get(c)\n",
        " \n",
        "    return s\n",
        "    \n",
        " \n",
        "#Final Testing\n",
        "if __name__ == '__main__':\n",
        "\n",
        "  val = input(\"Enter your cover text: \")\n",
        "  message= input(\"Enter secret message \")\n",
        "  tobits=buildHuffmanTree(message)\n",
        "  masked_stego = MaskedStego()\n",
        "  print(masked_stego(val, tobits, 3, 0.01))\n",
        "  print(\"Secret message to bits: \",tobits)\n",
        " \n",
        "  \n",
        "   "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKKmBy7_UQhX",
        "outputId": "773d48fc-e3c9-4a53-d72f-cad86301afb7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing roBERTa.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 roBERTa.py "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZYTpcIKU1F5",
        "outputId": "f5f1c296-6994-45bb-ee1b-1c4ddfa4c591"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "Enter your cover text: The internet has revolutionized the way we shop. Because of the numerous advantages and benefits, more and more people these days prefer purchasing things online than the conventional way of going to stores.What are some reasons many people love online shopping, and why is it so great? Below are the top ten reasons for shopping online.\n",
            "Enter secret message meet me at downtown at 9\n",
            "Downloading vocab.json: 100% 878k/878k [00:00<00:00, 2.38MB/s]\n",
            "Downloading merges.txt: 100% 446k/446k [00:00<00:00, 1.43MB/s]\n",
            "Downloading config.json: 100% 481/481 [00:00<00:00, 277kB/s]\n",
            "Downloading pytorch_model.bin: 100% 478M/478M [00:09<00:00, 53.3MB/s]\n",
            "[tensor(7977)]\n",
            "['Ġrevolution']\n",
            "replace 7977 ['Ġrevolution']\n",
            "[tensor(169)]\n",
            "['Ġway']\n",
            "replace 169 ['Ġway']\n",
            "[tensor(3047), tensor(14812)]\n",
            "['ĠBecause', 'ĠRegardless']\n",
            "part of bit sequence 0\n",
            "index 0\n",
            "replace 3047 ['ĠBecause']\n",
            "[tensor(171), tensor(3617), tensor(1337), tensor(4678), tensor(15785), tensor(276), tensor(10807), tensor(19539), tensor(776)]\n",
            "['Ġmany', 'Ġnumerous', 'Ġvarious', 'Ġobvious', 'Ġaforementioned', 'Ġsame', 'Ġcountless', 'Ġmyriad', 'Ġeconomic']\n",
            "part of bit sequence 001\n",
            "index 1\n",
            "replace 3617 ['Ġnumerous']\n",
            "[tensor(38940), tensor(1795), tensor(12340), tensor(9742), tensor(2019), tensor(39303), tensor(1042)]\n",
            "['Ġdisadvantages', 'Ġbenefits', 'Ġadvantages', 'Ġconvenience', 'Ġchallenges', 'Ġdrawbacks', 'Ġcosts']\n",
            "part of bit sequence 00\n",
            "index 0\n",
            "replace 38940 ['Ġdisadvantages']\n",
            "[tensor(55)]\n",
            "['Ġmore']\n",
            "replace 55 ['Ġmore']\n",
            "[tensor(360)]\n",
            "['Ġdays']\n",
            "replace 360 ['Ġdays']\n",
            "[tensor(383), tensor(3057), tensor(785), tensor(1964), tensor(402), tensor(20279), tensor(689)]\n",
            "['Ġthings', 'Ġgoods', 'Ġproducts', 'Ġitems', 'Ġsomething', 'Ġgroceries', 'Ġfood']\n",
            "part of bit sequence 10\n",
            "index 2\n",
            "replace 785 ['Ġproducts']\n",
            "[tensor(5), tensor(10), tensor(143)]\n",
            "['Ġthe', 'Ġa', 'Ġany']\n",
            "part of bit sequence 0\n",
            "index 0\n",
            "replace 5 ['Ġthe']\n",
            "[tensor(9)]\n",
            "['Ġof']\n",
            "replace 9 ['Ġof']\n",
            "[tensor(2326), tensor(2792), tensor(1400), tensor(210), tensor(6464), tensor(3482), tensor(1139), tensor(7819), tensor(1645), tensor(2304), tensor(2229)]\n",
            "['Ġstores', 'Ġshop', 'Ġstore', 'Ġmarket', 'Ġshops', 'Ġshopping', 'Ġtown', 'ĠWalmart', 'ĠAmazon', 'Ġretail', 'Ġpurchase']\n",
            "part of bit sequence 111\n",
            "index 7\n",
            "replace 7819 ['ĠWalmart']\n",
            "[tensor(2188), tensor(596), tensor(1219)]\n",
            "['Ġreasons', 'Ġwhy', 'Ġreason']\n",
            "part of bit sequence 0\n",
            "index 0\n",
            "replace 2188 ['Ġreasons']\n",
            "[tensor(6573), tensor(101), tensor(657), tensor(4402), tensor(634), tensor(2254), tensor(6218), tensor(304), tensor(2807), tensor(608), tensor(8348), tensor(5940)]\n",
            "['Ġprefer', 'Ġlike', 'Ġlove', 'Ġfavor', 'Ġusing', 'Ġenjoy', 'Ġenjoying', 'Ġuse', 'Ġchoose', 'Ġdoing', 'Ġchoosing', 'Ġrecommend']\n",
            "part of bit sequence 100\n",
            "index 4\n",
            "replace 634 ['Ġusing']\n",
            "[tensor(8)]\n",
            "['Ġand']\n",
            "replace 8 ['Ġand']\n",
            "[tensor(24), tensor(42), tensor(14)]\n",
            "['Ġit', 'Ġthis', 'Ġthat']\n",
            "part of bit sequence 0\n",
            "index 0\n",
            "replace 24 ['Ġit']\n",
            "[tensor(1398), tensor(1216), tensor(4421)]\n",
            "['ĠHere', 'ĠThese', 'ĠBelow']\n",
            "part of bit sequence 1\n",
            "index 1\n",
            "replace 1216 ['ĠThese']\n",
            "[tensor(299)]\n",
            "['Ġtop']\n",
            "replace 299 ['Ġtop']\n",
            "[tensor(13), tensor(596), tensor(9)]\n",
            "['Ġfor', 'Ġwhy', 'Ġof']\n",
            "part of bit sequence 0\n",
            "index 0\n",
            "replace 13 ['Ġfor']\n",
            "{'stego_text by roBERTa': 'The internet has revolutionized the way we shop. Because of the numerous advantages and disadvantages, more and more people these days prefer purchasing products online than the conventional way of going to Walmart.What are some reasons many people using online shopping, and why is it so great? These are the top ten reasons for shopping online.', 'encoded_message': '0001001001110100010'}\n",
            "Secret message to bits:  00010010011101000100011101111010011101110101100111101110101100011101111010010\n"
          ]
        }
      ]
    }
  ]
}