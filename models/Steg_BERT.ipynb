{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Steg-BERT.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ca0oPd35XyLq",
        "outputId": "a10bd16e-f466-4b0d-9d48-203dde8edcc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.21.1-py3-none-any.whl (4.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 19.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 49.7 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 6.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.8.1 pyyaml-6.0 tokenizers-0.12.1 transformers-4.21.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 5.6 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.97\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wordpiece\n",
            "  Downloading wordpiece-0.0.0.tar.gz (2.1 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: wordpiece\n",
            "  Building wheel for wordpiece (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wordpiece: filename=wordpiece-0.0.0-py3-none-any.whl size=2001 sha256=2ad7570ddb8c489f3b881bcf7a0bd12a16defc6a5907e0128fcc0135c9c35d76\n",
            "  Stored in directory: /root/.cache/pip/wheels/a9/dd/42/24eed7273d41969aab38c87fce5a092bf1bdfe1470450d4dd2\n",
            "Successfully built wordpiece\n",
            "Installing collected packages: wordpiece\n",
            "Successfully installed wordpiece-0.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "!pip install wordpiece"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Bert.py\n",
        "\n",
        "from typing import List, Union\n",
        "from io import StringIO\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import torch\n",
        "from torch import Tensor\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "from transformers.tokenization_utils import PreTrainedTokenizer\n",
        "import heapq\n",
        "from heapq import heappop, heappush\n",
        " \n",
        "   \n",
        "       \n",
        "class MaskedStegoByBert: \n",
        "        \n",
        "    def __init__(self,name='bert-base-cased'): #cased bert means that uppercase, accent markers are accepted \n",
        "        self._tokenizer: PreTrainedTokenizer  = BertTokenizer.from_pretrained(name)\n",
        "        self._model = BertForMaskedLM.from_pretrained(name)\n",
        "        self._STOPWORDS: List[str] = stopwords.words('english')\n",
        "\n",
        "    def __call__(self, cover_text, message, mask_interval = 3, score_threshold = 0.01): \n",
        "        assert set(message) <= set('01') #make sure that the binary message is a set of 0s and 1s\n",
        "        message_io = StringIO(message) \n",
        "        #pre-processing cover text inserted \n",
        "        #encode process goes through 4 steps normalization of cover text, pre-tokenizing, model, decoding. (similar to tokenizer.encode)\n",
        "        encoded_ids= self._tokenizer([cover_text],return_tensors='pt').input_ids[0] #returns a tensor for each word of the cover text, tensor is numerical representation for each word within the cover text\n",
        "        \n",
        "        masked_ids= self._mask(encoded_ids.clone(),mask_interval) #masked ids are the position embeddings for each word\n",
        "        sorted_score, indices= self._predict(masked_ids)\n",
        "        \n",
        "\n",
        "        processed= { 'input_ids': encoded_ids, 'masked_ids': masked_ids, 'sorted_output': (sorted_score, indices) }\n",
        "        \n",
        "        input_ids = processed['input_ids'] #single sequence: [CLS] + sentence ids + [SEP]\n",
        "        masked_ids = processed['masked_ids'] #1 for words and 0 for paddings\n",
        "        sorted_score, indices = processed['sorted_output']\n",
        "        for i_token, token in enumerate(masked_ids):\n",
        "            if token != self._tokenizer.mask_token_id:\n",
        "                continue\n",
        "            ids = indices[i_token]\n",
        "            scores = sorted_score[i_token]\n",
        "            #picks candidates according to their scores that must be of probability more than the set threshold 0.01 for encoding and 0.005 for decoding\n",
        "            candidates = self._pick_candidates_threshold(ids, scores, score_threshold)\n",
        "            print(candidates)\n",
        "            print(self._tokenizer.convert_ids_to_tokens(candidates)) #all suitable words to be placed in the [mask] position\n",
        "            \n",
        "            replace_token_id = self.Indexing(candidates, message_io).item() #perfect binary tree btakhod el candidates w btkhtar el suitable words for the bit sequence of our secret message\n",
        "            \n",
        "            print('replace', replace_token_id, self._tokenizer.convert_ids_to_tokens([replace_token_id]))\n",
        "            input_ids[i_token] = replace_token_id #hena byghyar el [Mask] tokens into the replace_token_id tensor to be converted to tokens in the decode part coming (tokenizer.decode)\n",
        "            \n",
        "        encoded_message: str = message_io.getvalue()[:message_io.tell()]\n",
        "        \n",
        "        message_io.close()\n",
        "        stego_text = self._tokenizer.decode(input_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "        #self._roBERTa(cover_text,message);\n",
        "        \n",
        "        return { 'stego_text by BERT': stego_text, 'encoded_message': encoded_message }\n",
        "\n",
        "    def decode(self, stego_text, mask_interval = 3, score_threshold = 0.005):\n",
        "        decoded_message: List[str] = []\n",
        "        #pre processing stego text.\n",
        "        encoded_ids= self._tokenizer([stego_text],return_tensors='pt').input_ids[0]\n",
        "        masked_ids= self._mask(encoded_ids.clone(),mask_interval)\n",
        "        \n",
        "        sorted_score, indices= self._predict(masked_ids) \n",
        "       \n",
        "        processed= { 'input_ids': encoded_ids, 'masked_ids': masked_ids, 'sorted_output': (sorted_score, indices) }\n",
        "        input_ids = processed['input_ids']\n",
        "        masked_ids = processed['masked_ids']\n",
        "        sorted_score, indices = processed['sorted_output']\n",
        "        for i_token, token in enumerate(masked_ids):\n",
        "            if token != self._tokenizer.mask_token_id: \n",
        "                continue\n",
        "            ids = indices[i_token]\n",
        "            scores = sorted_score[i_token]\n",
        "            candidates = self._pick_candidates_threshold(ids, scores, score_threshold)\n",
        "            if len(candidates) < 2: #if candidates equals 1 or less do nothing as there are more than 1 [mask] to be predicted so will be insufficient candidates so it has to be more than 1\n",
        "                continue\n",
        "            chosen_id: int = input_ids[i_token].item()\n",
        "            decoded_message.append(self._block_decode_single(candidates, chosen_id))\n",
        "        return {'decoded_message': ''.join(decoded_message)}\n",
        "\n",
        "   \n",
        "\n",
        "    def _mask(self, input_ids: Union[Tensor, List[List[int]]], mask_interval: int) -> Tensor:\n",
        "        length = len(input_ids)\n",
        "        tokens: List[str] = self._tokenizer.convert_ids_to_tokens(input_ids)\n",
        "        \n",
        "        offset = mask_interval // 2 + 2 #the offset specify the number of [mask] tokens according to the specified mask_interval \n",
        "        \n",
        "        mask_count = offset\n",
        "        for i, token in enumerate(tokens):\n",
        "            # Skip initial subword\n",
        "            if i + 1 < length and tokens[i + 1].startswith('##'): continue\n",
        "            if not self._substitutable_single(token): continue #skipping stopwords, subwords, and words that are not within the (a-z) letters.\n",
        "            if mask_count % mask_interval == 0: \n",
        "                input_ids[i] = self._tokenizer.mask_token_id #replace the value of this index (normal word) with the [mask] token whose input_id equals to 103   \n",
        "            mask_count += 1  \n",
        "       \n",
        "        return input_ids\n",
        "\n",
        "    \n",
        "    #forward neural network layer\n",
        "    def _predict(self, input_ids: Union[Tensor, List[List[int]]]): #this is the MLM (masked language model)\n",
        "        self._model.eval()\n",
        "        with torch.no_grad():\n",
        "            output = self._model(input_ids.unsqueeze(0))['logits'][0] #The predicted token_id is extracted from this logit (embedding vector resulting from last encoder layer in bert) using a softmax transformation.\n",
        "            #softmaxed score is applying softmax on the scores to focus on the ones with high scores\n",
        "            softmaxed_score = F.softmax(output, dim=1)  # [word_len, vocab_len]\n",
        "            return softmaxed_score.sort(dim=1, descending=True)\n",
        "\n",
        "    \n",
        "   \n",
        "    def _pick_candidates_threshold(self, ids: Tensor, scores: Tensor, threshold: float) -> List[int]:\n",
        "        filtered_ids: List[int] = ids[scores >= threshold]\n",
        "        def filter_fun(idx: Tensor) -> bool:\n",
        "            return self._substitutable_single(self._tokenizer.convert_ids_to_tokens(idx.item()))\n",
        "        return list(filter(filter_fun, filtered_ids))\n",
        "\n",
        "    def _substitutable_single(self, token: str) -> bool:\n",
        "        if token.startswith('##'): return False\n",
        "        if token.lower() in self._STOPWORDS: return False\n",
        "        if not token.isalpha(): return False\n",
        "        return True\n",
        "\n",
        "    @staticmethod\n",
        "    def Indexing(ids: List[int], message: StringIO) -> int: \n",
        "        assert len(ids) > 0\n",
        "        if len(ids) == 1:\n",
        "            return ids[0]\n",
        "        capacity = len(ids).bit_length() - 1\n",
        "        #print(capacity)\n",
        "        bits_str = message.read(capacity)\n",
        "        print(\"Secret message bit sequence chunk: \",bits_str)\n",
        "        if len(bits_str) < capacity:\n",
        "            padding: str = '0' * (capacity - len(bits_str))\n",
        "            bits_str = bits_str + padding\n",
        "            message.write(padding)\n",
        "        index = int(bits_str, 2) #from binary to number\n",
        "        #print(\"index\",index)\n",
        "        return ids[index]\n",
        "\n",
        "    @staticmethod\n",
        "    def _block_decode_single(ids: List[int], chosen_id: int) -> str:\n",
        "        if len(ids) < 2:\n",
        "            return ''\n",
        "        capacity = len(ids).bit_length() - 1\n",
        "        index = ids.index(chosen_id)\n",
        "        return format(index, '0' + str(capacity) +'b')\n",
        "\n",
        "\n",
        "def isLeaf(root):\n",
        "    return root.left is None and root.right is None\n",
        " \n",
        " \n",
        "# A Tree node\n",
        "class Node:\n",
        "    def __init__(self, ch, freq, left=None, right=None):\n",
        "        self.ch = ch\n",
        "        self.freq = freq\n",
        "        self.left = left\n",
        "        self.right = right\n",
        " \n",
        "    \n",
        "    def __lt__(self, other):\n",
        "        return self.freq < other.freq\n",
        " \n",
        " \n",
        "# Traverse the Huffman Tree and store Huffman Codes in a dictionary\n",
        "def encode(root, s, huffman_code):\n",
        " \n",
        "    if root is None:\n",
        "        return\n",
        " \n",
        "    # found a leaf node\n",
        "    if isLeaf(root):\n",
        "        huffman_code[root.ch] = s if len(s) > 0 else '1'\n",
        " \n",
        "    encode(root.left, s + '0', huffman_code)\n",
        "    encode(root.right, s + '1', huffman_code)\n",
        " \n",
        " \n",
        "\n",
        " \n",
        " \n",
        "# Builds Huffman Tree and decodes the given input text\n",
        "def buildHuffmanTree(text):\n",
        " \n",
        "    # base case: empty string\n",
        "    if len(text) == 0:\n",
        "        return\n",
        " \n",
        "    # count the frequency of appearance of each character\n",
        "    # and store it in a dictionary\n",
        "    freq = {i: text.count(i) for i in set(text)}\n",
        " \n",
        "    # Create a priority queue to store live nodes of the Huffman tree.\n",
        "    pq = [Node(k, v) for k, v in freq.items()]\n",
        "    heapq.heapify(pq)\n",
        " \n",
        "    # do till there is more than one node in the queue\n",
        "    while len(pq) != 1:\n",
        " \n",
        "        # Remove the two nodes of the highest priority\n",
        "        # (the lowest frequency) from the queue\n",
        " \n",
        "        left = heappop(pq)\n",
        "        right = heappop(pq)\n",
        " \n",
        "        # create a new internal node with these two nodes as children and\n",
        "        # with a frequency equal to the sum of the two nodes' frequencies.\n",
        "        # Add the new node to the priority queue.\n",
        " \n",
        "        total = left.freq + right.freq\n",
        "        heappush(pq, Node(None, total, left, right))\n",
        " \n",
        "    # `root` stores pointer to the root of Huffman Tree\n",
        "    root = pq[0]\n",
        " \n",
        "    # traverse the Huffman tree and store the Huffman codes in a dictionary\n",
        "    huffmanCode = {}\n",
        "    encode(root, '', huffmanCode)\n",
        " \n",
        " \n",
        "    # print the encoded string\n",
        "    s = ''\n",
        "    for c in text:\n",
        "        s += huffmanCode.get(c)\n",
        " \n",
        "    return s;\n",
        "\n",
        "\n",
        "#Final Testing\n",
        "if __name__ == '__main__':\n",
        "\n",
        "  val = input(\"Enter your cover text/stego text: \")\n",
        "  message= input(\"Enter secret message \")\n",
        "  tobits=buildHuffmanTree(message)\n",
        "  print('Bit sequence of secret message: ',tobits)\n",
        "  masked_stego = MaskedStegoByBert()\n",
        "  print(masked_stego(val, tobits, 3, 0.01))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZfKz7HeSX9D0",
        "outputId": "3a6d8dbd-b508-48f3-99b1-0cf6a4170c5e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing Bert.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 Bert.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cu3tPjIbYRw5",
        "outputId": "d851be38-2358-4013-e940-e3bc1cc720bb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "Enter your cover text/stego text:  The internet has revolutionized the way we shop. Because of the numerous advantages and benefits, more and more people these days prefer purchasing things online than the conventional way of going to stores.What are some reasons many people love online shopping, and why is it so great? Below are the top ten reasons for shopping online.\n",
            "Enter secret message meet me at downtown at 9\n",
            "Bit sequence of secret message:  11001001001010111001000111011010111111000001111010100000111100111011010111110\n",
            "Downloading vocab.txt: 100% 208k/208k [00:00<00:00, 2.75MB/s]\n",
            "Downloading tokenizer_config.json: 100% 29.0/29.0 [00:00<00:00, 23.6kB/s]\n",
            "Downloading config.json: 100% 570/570 [00:00<00:00, 423kB/s]\n",
            "Downloading pytorch_model.bin: 100% 416M/416M [00:06<00:00, 62.6MB/s]\n",
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[tensor(4639), tensor(7210)]\n",
            "['Internet', 'internet']\n",
            "Secret message bit sequence chunk:  1\n",
            "replace 7210 ['internet']\n",
            "[tensor(1242), tensor(1672), tensor(1896), tensor(2670), tensor(1207), tensor(12675), tensor(2567), tensor(3321), tensor(6047), tensor(5119)]\n",
            "['many', 'various', 'added', 'economic', 'new', 'technological', 'numerous', 'huge', 'vast', 'obvious']\n",
            "Secret message bit sequence chunk:  100\n",
            "replace 1207 ['new']\n",
            "[tensor(1234), tensor(1535)]\n",
            "['people', 'women']\n",
            "Secret message bit sequence chunk:  1\n",
            "replace 1535 ['women']\n",
            "[tensor(9241), tensor(4147), tensor(13980), tensor(6001), tensor(4006)]\n",
            "['buying', 'selling', 'purchasing', 'shopping', 'finding']\n",
            "Secret message bit sequence chunk:  00\n",
            "replace 9241 ['buying']\n",
            "[tensor(2361), tensor(7228), tensor(4400), tensor(2999), tensor(1237), tensor(17780), tensor(2030)]\n",
            "['traditional', 'conventional', 'usual', 'normal', 'American', 'cheaper', 'modern']\n",
            "Secret message bit sequence chunk:  10\n",
            "replace 4400 ['usual']\n",
            "[tensor(1250), tensor(6001), tensor(4130), tensor(1278), tensor(2319), tensor(7116), tensor(4822), tensor(1671)]\n",
            "['work', 'shopping', 'shop', 'school', 'market', 'shops', 'stores', 'business']\n",
            "Secret message bit sequence chunk:  010\n",
            "replace 4130 ['shop']\n",
            "[tensor(1234), tensor(4038), tensor(1535)]\n",
            "['people', 'Americans', 'women']\n",
            "Secret message bit sequence chunk:  1\n",
            "replace 4038 ['Americans']\n",
            "[tensor(6001)]\n",
            "['shopping']\n",
            "replace 6001 ['shopping']\n",
            "[tensor(1995), tensor(1421), tensor(1210), tensor(1160), tensor(1300), tensor(1565)]\n",
            "['ten', 'five', 'three', 'two', 'four', 'six']\n",
            "Secret message bit sequence chunk:  01\n",
            "replace 1421 ['five']\n",
            "[tensor(3294)]\n",
            "['online']\n",
            "replace 3294 ['online']\n",
            "{'stego_text by BERT': 'The internet has revolutionized the way we shop. Because of the new advantages and benefits, more and more women these days prefer buying things online than the usual way of going to shop. What are some reasons many Americans love online shopping, and why is it so great? Below are the top five reasons for shopping online.', 'encoded_message': '110010010010101'}\n"
          ]
        }
      ]
    }
  ]
}